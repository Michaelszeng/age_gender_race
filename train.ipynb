{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d68c3538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\n",
    "ETHNICITIES = { 0: \"White\", 1: \"Black\", 2: \"Asian\", 3: \"Indian\", 4: \"Hispanic\" }\n",
    "GENDERS = { 0: \"Male\", 1: \"Female\" }\n",
    "\"\"\"\n",
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install pandas\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import csv\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import FacesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb9090cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating the Neural Network Class\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Input: 48 x 48 x 1 = 2304\n",
    "        ### Inputs to Conv2d: Incoming layers, outgoing layers, Frame size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3)  #48 --> 46\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  #46 --> 23\n",
    "        self.conv2 = nn.Conv2d(2, 4, 3, stride=2)  #23 --> 11\n",
    "        self.conv3 = nn.Conv2d(4, 8, 3, stride=2) #11 --> 5\n",
    "\n",
    "        # Activation function to use\n",
    "        self.activation = F.relu\n",
    "\n",
    "        self.fc1 = nn.Linear(200, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = x.view(-1, 200)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29b38dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setting up neural network parameters, defining accuracy function\"\"\"\n",
    "batch_size = 32\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10\n",
    "\n",
    "data_path = \"age_gender.csv\"\n",
    "\n",
    "dataset = FacesDataset(csv_file=data_path)\n",
    "num_data_items = dataset.__len__()\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [int(num_data_items*0.8), int(num_data_items*0.2)])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net()\n",
    "# model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Checks accuracy of the model by running it on the data in the dataloader passed as a parameter\n",
    "    \n",
    "    The closer the returned score is to 0, the better\n",
    "    \"\"\"\n",
    "    num_correct_age = 0\n",
    "    num_correct_race = 0\n",
    "    num_correct_gender = 0\n",
    "    num_samples = loader.__len__()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():  #speeds up process\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)   #shape: (32, 48, 48)\n",
    "            y = y.to(device=device)   #shape: (32, 3)\n",
    "            \n",
    "            #GETTING AND FORMATTING MODEL PREDICTIONS\n",
    "            x = x.unsqueeze(1)   #Add a dimension to the tensor for number of channels (which is 1)\n",
    "            scores = model(x.float())   #Shape: (32, 3)\n",
    "#             print(\"scores: \" + str(scores))\n",
    "            \n",
    "            #GETTING NUM CORRECT\n",
    "#             race = torch.round(scores[:,1].clone())    #Getting integer-rounded copy of races\n",
    "#             gender = torch.round(scores[:,2].clone())  #Getting integer-rounded copy of gender\n",
    "#             scores = torch.column_stack((torch.round(scores[:,0]), race))  #recreating 32x3 tensor with integer rounded age, race, and gender\n",
    "#             scores = torch.column_stack((scores, gender))\n",
    "#             print(\"scores: \" + str(scores))\n",
    "#             print(\"y: \" + str(y))\n",
    "            \n",
    "            #CALCULATING ACCURACY SCORE\n",
    "            age_diffs = torch.abs(scores[:,0] - y[:,0])\n",
    "            race_diffs = torch.abs(scores[:,1] - y[:,1])\n",
    "            gender_diffs = torch.abs(scores[:,2] - y[:,2])\n",
    "            age_diff_avg = torch.mean(age_diffs)\n",
    "            race_diff_avg = torch.mean(race_diffs)\n",
    "            gender_diff_avg = torch.mean(gender_diffs)\n",
    "            \n",
    "            age_std = torch.std(age_diffs)\n",
    "            race_std = torch.std(race_diffs)\n",
    "            gender_std = torch.std(gender_diffs)\n",
    "            \n",
    "            age_accuracy = age_diff_avg/age_std\n",
    "            race_accuracy = race_diff_avg/race_std\n",
    "            gender_accuracy = gender_diff_avg/gender_std\n",
    "            total_accuracy = (age_accuracy + race_accuracy + gender_accuracy) / 3\n",
    "\n",
    "#             print(\"------------------------\")\n",
    "#             print(\"age_accuracy: \" + str(age_accuracy))\n",
    "#             print(\"race_accuracy: \" + str(race_accuracy))\n",
    "#             print(\"gender_accuracy: \" + str(gender_accuracy))\n",
    "#             print(\"total_accuracy: \" + str(total_accuracy))\n",
    "#             print(\"------------------------\")\n",
    "        \n",
    "#         print(f'accuracy score (the closer to zero, the better): {total_accuracy}')\n",
    "    return total_accuracy\n",
    "\n",
    "\n",
    "def scale_scores(data):\n",
    "    c0 = data[:,0]\n",
    "    c1 = data[:,1]\n",
    "    c2 = data[:,2]\n",
    "    mean = torch.mean(c0)\n",
    "    transformed0 = c0/mean\n",
    "    mean = torch.mean(c1)\n",
    "    transformed1 = c1/mean\n",
    "    mean = torch.mean(c2)\n",
    "    transformed2 = c2/mean\n",
    "    transformed = torch.column_stack((transformed0, transformed1))\n",
    "    transformed = torch.column_stack((transformed, transformed2))\n",
    "#     print(transformed)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64545f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.7361786794702682, Train Acc: 132.30091857910156, Test Acc: 208.58447265625\n",
      "Epoch: 2, Loss: 0.6430093073623779, Train Acc: 157.34471130371094, Test Acc: 152.08395385742188\n",
      "Epoch: 3, Loss: 0.6182443788542112, Train Acc: 213.51182556152344, Test Acc: 205.20269775390625\n",
      "Epoch: 4, Loss: 0.6083486533989191, Train Acc: 154.328857421875, Test Acc: 312.5765686035156\n",
      "Epoch: 5, Loss: 0.5820909787289811, Train Acc: 154.19679260253906, Test Acc: 188.3456268310547\n",
      "Epoch: 6, Loss: 0.5767212998103131, Train Acc: 189.1984405517578, Test Acc: 225.1914520263672\n",
      "Epoch: 7, Loss: 0.5669590545092626, Train Acc: 185.26722717285156, Test Acc: 224.02627563476562\n",
      "Epoch: 8, Loss: 0.5560002037485776, Train Acc: 166.9034881591797, Test Acc: 190.71401977539062\n",
      "Epoch: 9, Loss: 0.5488405347775046, Train Acc: 138.7461700439453, Test Acc: 210.38368225097656\n",
      "Epoch: 10, Loss: 0.5412753394994929, Train Acc: 151.29226684570312, Test Acc: 181.66607666015625\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training\"\"\"\n",
    "\n",
    "all_losses = []\n",
    "all_train_acc = []\n",
    "all_test_acc = []\n",
    "for epoch in range(num_epochs):  #Looping through epochs\n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):   #Looping through batches\n",
    "        #Get data to cuda if possible\n",
    "        data = data.to(device=device)             #data is a torch tensor of 32 48x48 images (shape=[32,48,48])\n",
    "        data = data.unsqueeze(1)                  #Add a dimension to the tensor for number of channels (which is 1)\n",
    "#         print(\"data.shape: \" + str(data.shape))\n",
    "        targets = targets.to(device=device)       #data is a torch tensor of 32 sets of 3 annotations (shape=[32,3])\n",
    "#         print(\"tagrets.shape: \" + str(targets.shape))\n",
    "#         print(\"data[0,:,:].shape: \" + str(np.array(data[0,:,:].shape)))\n",
    "#         cv2.imwrite(\"test.png\", np.array(data[0,:,:]))\n",
    "        \n",
    "        #Forward prop\n",
    "        scores = model(data.float())\n",
    "        \n",
    "        #Scale scores and targets so all entries are between 0 and 1\n",
    "        scores_scaled = scale_scores(scores)\n",
    "        targets_scaled = scale_scores(targets)\n",
    "        \n",
    "#         print(\"scores: \" + str(scores))\n",
    "#         print(\"scores_scaled: \" + str(scores_scaled))\n",
    "#         print(\"targets: \" + str(targets))\n",
    "#         print(\"targets_scaled: \" + str(targets_scaled))\n",
    "        loss = criterion(scores_scaled, targets_scaled)\n",
    "#         print(loss)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        #Backward prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  #gradient descent\n",
    "    \n",
    "    l = sum(losses)/len(losses)\n",
    "    all_losses.append(l)\n",
    "    train_acc = check_accuracy(train_loader, model)\n",
    "    all_train_acc.append(train_acc)\n",
    "    test_acc = check_accuracy(test_loader, model)\n",
    "    all_test_acc.append(test_acc)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {l}, Train Acc: {train_acc}, Test Acc: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "293bb978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Plotting Results\"\"\"\n",
    "\n",
    "# print(\"Checking final accuracy on training data.\")\n",
    "# check_accuracy(train_loader, model)\n",
    "\n",
    "# print(\"Checking final accuracy on testing data.\")\n",
    "# check_accuracy(test_loader, model)\n",
    "\n",
    "\n",
    "\n",
    "def plot(loss, train_acc, test_acc):\n",
    "    plt.scatter(range(0, len(all_losses)), loss)\n",
    "    plt.scatter(range(0, len(train_acc)), train_acc)\n",
    "    plt.scatter(range(0, len(test_acc)), test_acc)\n",
    "    plt.plot(range(0, len(all_losses)), loss)\n",
    "    plt.plot(range(0, len(train_acc)), train_acc)\n",
    "    plt.plot(range(0, len(test_acc)), test_acc)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d,%m,%Y_%H,%M,%S\")\n",
    "    plt.savefig('plots/loss_accuracy_' + dt_string + '.png')\n",
    "    \n",
    "plot(all_losses, all_train_acc, all_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f576d63",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-274009003dc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age: %f     race: %f     gender: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmake_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_face.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-274009003dc0>\u001b[0m in \u001b[0;36mmake_pred\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age: %f     race: %f     gender: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael_zeng\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-15c9c7ae3d50>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael_zeng\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael_zeng\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael_zeng\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (numpy.ndarray, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "def make_pred(image, model):\n",
    "    \"\"\"\n",
    "    Makes prediction on a single image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scores = model(image)\n",
    "    print(\"age: %f     race: %f     gender: %f\" % (scores[0], scores[1], scores[2]))\n",
    "\n",
    "make_pred(cv2.imread(\"test_face.jpg\"), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b5745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
